# Calibration-Aware Conformal Decomposition (CACD): Complete Research Framework

## UPDATE (Important Clarification)

**What This Document Contains**:
- **Part I**: Initial brainstorming ideas from Claude Chat (5 different approaches) - these were PRELIMINARY thoughts
- **Part II**: The ACTUAL framework we're implementing - **CACD** - which Claude Chat synthesized from the best parts of all 5 ideas
- **Focus**: We are implementing **CACD (Part II)** - this is our novel contribution, NOT the individual ideas in Part I

**The Journey**:
1. Claude Chat first proposed 5 different creative approaches to uncertainty decomposition
2. After analysis, Claude Chat synthesized these into a single, coherent framework: **CACD**
3. CACD combines the best elements: heteroscedastic meta-learning (Idea 5) + influence functions (Idea 3) + temporal consistency (Idea 2)
4. **CACD is what we're building** - it's the refined, complete framework

---

## Executive Summary

**Core Innovation**: Transform the passive calibration set in conformal prediction into an active learning resource for uncertainty decomposition. This framework combines the best of conformal prediction (distribution-free guarantees) with modern uncertainty quantification (interpretable decomposition) through a principled use of calibration data that has been completely overlooked in prior work.

**Key Differentiator**: Unlike existing methods that require model retraining or ensembles, CACD works with ANY pre-trained model and provides both aleatoric and epistemic uncertainty from a SINGLE forward pass while maintaining exact finite-sample coverage guarantees.

---

## Part I: Initial Brainstorming Ideas (For Historical Context)

*Note: These were the initial creative proposals from Claude Chat. They served as inspiration but are NOT what we're implementing. We're implementing the synthesized CACD framework in Part II.*

### Idea 1: Conformal Uncertainty Flow Fields

**Concept**: Treat uncertainty as a vector field over the input space.

**Mathematical Framework**:
- Define flow field: $\vec{U}(x) = \nabla_x s(x, f(x))$ where $s$ is the conformal score
- Divergence reveals uncertainty sources: $\nabla \cdot \vec{U}(x) > 0$ â†’ epistemic (expanding uncertainty)
- Curl indicates aleatoric patterns: $\nabla \times \vec{U}(x) \neq 0$ â†’ data-driven rotations

**Strengths**: Beautiful geometric intuition, visualizable
**Weaknesses**: Computationally heavy, gradient instability in high dimensions
**Verdict**: Keep the geometric intuition but simplify computation

### Idea 2: Temporal Conformal Dynamics

**Concept**: For video/sequential data, decompose uncertainty evolution over time.

**Mathematical Framework**:
```
s(x_t, y_t) = s_0(x_t) + Î£_k Î±_k cos(Ï‰_k t + Ï†_k)
```
- Low frequencies (Ï‰_k small) â†’ epistemic (slow model confusion)
- High frequencies (Ï‰_k large) â†’ aleatoric (rapid noise fluctuations)
- Fourier analysis on calibration trajectories

**Strengths**: PERFECT for MOT17 dataset, natural for video
**Weaknesses**: Fourier decomposition might be hard to prove rigorously
**Verdict**: Use temporal consistency but with simpler formulation

### Idea 3: Conformal Influence Functions

**Concept**: Each calibration point influences the conformal set differently.

**Mathematical Framework**:
- Leave-one-out influence: $I_i(x_test) = |q_Î± - q_{Î±,-i}|$
- Influence variance â†’ epistemic uncertainty
- Stable influence â†’ aleatoric uncertainty
- Weight by distance: $w_i(x) = exp(-d(x, x_i)/h) Â· I_i(x)$

**Strengths**: THIS IS GOLD! Direct interpretability, reveals which calibration samples matter
**Weaknesses**: O(n_cal) computation for exact leave-one-out
**Verdict**: Core component of our framework with efficient approximation

### Idea 4: Multi-Resolution Conformal Decomposition

**Concept**: Different uncertainty scales at different resolutions.

**Mathematical Framework**:
- Coarse resolution: $s_coarse = |y - f_Î¸(downsample(x))|$ â†’ epistemic
- Fine resolution: $s_fine = |y - f(x)|$ â†’ total
- Difference: $s_fine - s_coarse$ â†’ aleatoric

**Strengths**: Clean separation idea
**Weaknesses**: Parametric model too simplistic, requires multiple models
**Verdict**: Use the multi-scale intuition in feature space instead

### Idea 5: Conformal Score Residual Networks

**Concept**: Train a meta-predictor on calibration data to predict conformal scores.

**Mathematical Framework**:
```python
# Train g_Ï† on calibration set
g_Ï†: X â†’ (Î¼_s, Ïƒ_sÂ²)  # Predict score distribution
Loss = E[(s_i - Î¼_s(x_i))Â²/2Ïƒ_sÂ²(x_i) + log Ïƒ_sÂ²(x_i)/2]
```
- Î¼_s(x) â†’ aleatoric (predictable component)
- Ïƒ_sÂ²(x) â†’ epistemic (uncertainty about score)

**Strengths**: BRILLIANT! Actually uses calibration data actively
**Weaknesses**: Need careful regularization
**Verdict**: Core innovation - heteroscedastic meta-learning on calibration

---

## Part II: The CACD Framework - THIS IS WHAT WE'RE IMPLEMENTING

**IMPORTANT**: This is the actual framework we are building. Everything below represents our novel contribution.

### Core Architecture: Three-Stage Pipeline

```
Stage 1: Calibration Embedding Network (from Idea 5)
    â†“
Stage 2: Influence-Weighted Decomposition (from Idea 3)
    â†“
Stage 3: Temporal Consistency Validation (from Idea 2)
```

### 1. Mathematical Foundation

#### 1.1 Setting
Given:
- Pre-trained model $f: \mathcal{X} \to \mathcal{Y}$ (FROZEN - no retraining)
- Calibration set $\mathcal{D}_{cal} = \{(x_j, y_j)\}_{j=1}^{n_{cal}}$
- Test point $(x_{test}, y_{test})$ where $y_{test}$ is unknown

#### 1.2 Core Innovation
Instead of using calibration set PASSIVELY (just computing quantile), we ACTIVELY learn uncertainty structure from it:
- Dense calibration regions â†’ Low epistemic uncertainty
- Sparse calibration regions â†’ High epistemic uncertainty
- High residual variance â†’ High aleatoric uncertainty
- Low residual variance â†’ Low aleatoric uncertainty

### 2. Stage 1: Calibration Score Prediction Network

#### 2.1 Heteroscedastic Meta-Predictor
Train neural network $g_\theta: \mathcal{X} \to \mathbb{R}^2$ on calibration set:

```
g_Î¸(x) = (Î¼_s(x), log Ïƒ_sÂ²(x))
```

where:
- $\mu_s(x)$ = predicted expected conformal score (aleatoric component)
- $\sigma_s^2(x)$ = predicted variance of conformal score (epistemic component)

#### 2.2 Heteroscedastic Loss
```
L_het(Î¸) = (1/n_cal) Î£_j [(s_j - Î¼_s(x_j))Â²/(2Ïƒ_sÂ²(x_j)) + log Ïƒ_sÂ²(x_j)/2]
```

This loss simultaneously learns:
1. Mean predictor (where errors are expected)
2. Variance predictor (where we're uncertain about errors)

### 3. Stage 2: Influence-Based Refinement

#### 3.1 Influence Weights
For each calibration point $x_j$, compute influence on test point:

```
w_j(x_test) = exp(-d_F(x_test, x_j)/h) Â· I_j(x_test)
```

where:
- $d_F$ = distance in feature space (penultimate layer of f)
- $h$ = bandwidth (cross-validated)
- $I_j$ = influence of removing calibration point j

#### 3.2 Influence Function Computation
```
I_j(x_test) = |âˆ‚q_Î±/âˆ‚Îµ_j|_{Îµ=0}
```

Efficient approximation:
```
I_j â‰ˆ (1/n_cal) Â· ðŸ™[s_j â‰ˆ q_Î±] Â· K_h(x_test, x_j)
```

#### 3.3 Refined Epistemic Uncertainty
```
ÏƒÌƒÂ²_epis(x_test) = ÏƒÂ²_s(x_test) Â· Var_j[w_j(x_test) Â· s_j]
```

Key insight: Influence variance reveals model uncertainty!

### 4. Stage 3: Temporal Coherence (For Video)

#### 4.1 Temporal Loss
For consecutive frames in video:
```
L_temp = Î£_t [||d/dt ÏƒÂ²_epis(x_t)||Â² + Î» Â· TV(ÏƒÂ²_alea(x_t))]
```

This enforces:
- Epistemic uncertainty changes smoothly (model knowledge continuous)
- Aleatoric uncertainty can jump (occlusions are sudden)

#### 4.2 MOT17-Specific Formulation
For bounding box b_t at frame t:
```
ÏƒÂ²_alea(b_t) = E_Ï„[IoU(f_Ï„(x_t), b_t)]  # Variance across augmentations
ÏƒÂ²_epis(b_t) = exp(-Î£_k sim(h(x_t), h(x_k^cal))/k)  # Feature similarity to calibration
```

### 5. Theoretical Guarantees

#### 5.1 Coverage Guarantee
**Theorem**: The prediction interval
```
C_Î±(x) = [f(x) Â± q_Î± Â· âˆš(ÏƒÂ²_alea(x) + ÏƒÌƒÂ²_epis(x))]
```
satisfies:
```
P(y âˆˆ C_Î±(x)) â‰¥ 1 - Î±
```

**Proof Sketch**:
- Exchangeability preserved through monotonic transformation
- Quantile computation maintains finite-sample validity
- Decomposition doesn't break coverage (only refines intervals)

#### 5.2 Orthogonality Result
**Theorem**: As n_cal â†’ âˆž:
```
Corr(ÏƒÂ²_alea(x), ÏƒÌƒÂ²_epis(x)) â†’ 0
```

**Proof Sketch**:
- Aleatoric depends on p(y|x) (data distribution)
- Epistemic depends on p_cal(x) (calibration density)
- These are independent as calibration size grows

#### 5.3 Information Gain
**Theorem**: Information gain from decomposition:
```
I(U_alea, U_epis; D_cal) = H[s] - H[s | Î¼_s, Ïƒ_sÂ²] â‰¥ 0
```

This quantifies how much we learn from calibration!

### 6. Implementation Algorithm

```python
class CACD:
    def __init__(self):
        self.meta_network = HeteroscedasticMLP()
        self.influence_computer = InfluenceFunction()

    def calibrate(self, cal_features, cal_labels):
        # Stage 1: Train meta-predictor
        scores = compute_conformal_scores(cal_features, cal_labels)
        self.meta_network.fit(cal_features, scores)

        # Stage 2: Compute influence matrix
        self.influence_matrix = self.influence_computer(cal_features, scores)

        # Stage 3: Set quantiles
        self.q_alea = quantile(scores * cos(theta_alea), 1-alpha)
        self.q_epis = quantile(scores * sin(theta_epis), 1-alpha)

    def predict_uncertainty(self, test_features):
        # Get score distribution
        mu_s, sigma_s = self.meta_network(test_features)

        # Compute influences
        influences = self.influence_matrix @ test_features
        influence_var = var(influences)

        # Decompose
        aleatoric = mu_s  # Predictable error
        epistemic = sigma_s * sqrt(influence_var)  # Unpredictable Ã— influence

        return {
            'aleatoric': aleatoric,
            'epistemic': epistemic,
            'total': sqrt(aleatoric**2 + epistemic**2),
            'interval': self.compute_interval(test_features, aleatoric, epistemic)
        }
```

### 7. Network Architecture Details

#### 7.1 Meta-Network Structure
```
Input: Feature vector Ï†(x) from pre-trained model
    â†“
Linear(d â†’ 128) â†’ ReLU â†’ Dropout(0.1)
    â†“
Linear(128 â†’ 64) â†’ ReLU â†’ Dropout(0.1)
    â†“
Linear(64 â†’ 32) â†’ ReLU
    â†“
Split into two heads:
    Mean head: Linear(32 â†’ 1) â†’ Î¼_s
    Variance head: Linear(32 â†’ 1) â†’ log Ïƒ_sÂ²
```

#### 7.2 Training Strategy
**Phase 1** (50 epochs): Train mean only (ÏƒÂ² = 1)
**Phase 2** (30 epochs): Train variance only (Î¼ fixed)
**Phase 3** (20 epochs): Joint fine-tuning

Learning rates: Î¼ (1e-3 â†’ 5e-4), ÏƒÂ² (1e-4 â†’ 5e-5)

### 8. Dual Calibration Strategy

**Key Innovation**: Use TWO calibration sets!
1. **Dense set** D_cal^dense: For learning aleatoric patterns
2. **Sparse set** D_cal^sparse: For revealing epistemic boundaries

Train on dense, evaluate influence on sparse!

---

## Part III: Toy Problem Specifications

### Toy Problem 1: 2D Heteroscedastic Regression

#### Data Generation
```python
# Input distribution (mixture of 3 Gaussians with gaps)
x ~ 0.4Â·N([0,0], I) + 0.3Â·N([3,3], 0.5I) + 0.3Â·N([-3,3], 0.5I)

# Response with state-dependent noise
y = f*(x) + Ïƒ(x)Â·Îµ

where:
f*(xâ‚, xâ‚‚) = 5sin(2Ï€xâ‚) + 3cos(3Ï€xâ‚‚) + xâ‚xâ‚‚
Ïƒ(xâ‚, xâ‚‚) = 0.1 + 0.4Â·exp(-2(xâ‚Â² + xâ‚‚Â²))  # High noise at origin
Îµ ~ N(0, 1)
```

#### Expected Behavior
- **Aleatoric**: High near origin (Ïƒ large), low at boundaries
- **Epistemic**: High in gaps between Gaussians (no calibration data)

#### Validation Metrics
1. Coverage stratified by uncertainty type
2. Correlation between predicted and true uncertainties
3. Interval width efficiency

### Toy Problem 2: Temporal Tracking Simulation

#### State Evolution
```python
# Linear dynamics with oscillation
x_{t+1} = AÂ·x_t + BÂ·u_t + w_t

A = [[0.98, 0.1], [-0.1, 0.98]]  # Slight rotation
w_t ~ N(0, Q(x_t))  # State-dependent noise

# Occlusion model
visible_t ~ Bernoulli(p_visible(x_t))
y_t = CÂ·x_t + v_t if visible_t else MISSING
```

#### Expected Behavior
- **During occlusion**: Epistemic â†‘ (model unsure)
- **After occlusion**: Aleatoric â†‘ temporarily (motion uncertainty)
- **Stable tracking**: Both low

### Toy Problem 3: Classification with Reject Option

#### Setup
- MNIST-like data with ambiguous digits (3 vs 8, 4 vs 9)
- Add varying levels of noise to different classes
- Create "out-of-distribution" digits by blending

#### Expected Behavior
- **Clear digits**: Low both uncertainties
- **Noisy digits**: High aleatoric, low epistemic
- **OOD digits**: Low aleatoric, high epistemic
- **Noisy OOD**: High both

---

## Part IV: Experimental Design

### Experiment 1: Ablation Study
```
Baseline: Standard conformal prediction
+Meta-predictor: Add heteroscedastic network
+Influence: Add influence weighting
+Temporal: Add temporal consistency (for video)
```

Measure: Coverage, interval width, decomposition quality

### Experiment 2: Calibration Set Size
```
n_cal âˆˆ {50, 100, 200, 500, 1000, 2000}
```
Plot: Decomposition quality vs n_cal, theoretical vs empirical rates

### Experiment 3: Feature Space Comparison
```
Features from: {Last layer, Middle layer, Early layer, PCA, t-SNE}
```
Find: Optimal feature representation for uncertainty

### Experiment 4: Comparison with Baselines
- **EPICSCORE**: Requires ensemble
- **Deep Ensembles**: Multiple models
- **MC Dropout**: Multiple forward passes
- **Standard CP**: No decomposition
- **Ours**: Single model, single pass, with decomposition

---

## Part V: Visualization Suite

### Core Visualizations

#### V1: Uncertainty Heatmaps (2D problems)
- 3 panels: Total, Aleatoric, Epistemic
- Overlay calibration points
- Contour lines at quantiles

#### V2: Decomposition Scatter
- X: ÏƒÂ²_alea, Y: ÏƒÂ²_epis
- Color: Total uncertainty
- Size: Prediction error
- Ideal: Orthogonal spread

#### V3: Influence Network
- Nodes: Calibration points
- Edges: Influence weights
- Node size: Average influence
- Edge width: Influence strength

#### V4: Temporal Evolution (Video)
- Timeline of uncertainties
- Shaded regions: Occlusions
- Correlation with tracking errors

#### V5: Coverage Stratification
- 3Ã—3 grid: {Low, Med, High} Ã— {Alea, Epis}
- Bar height: Empirical coverage
- Red line: Nominal 1-Î±

---

## Part VI: Key Innovations Summary

### What Makes This CVPR/NeurIPS-Worthy

1. **First heteroscedastic meta-learning on calibration data**
   - Nobody has trained networks to predict conformal scores!
   - Elegant use of "wasted" calibration data

2. **Influence functions for epistemic uncertainty**
   - Novel application to conformal prediction
   - Provides interpretability: which calibration points matter

3. **Provable orthogonal decomposition**
   - Theoretical guarantee of independence as n_cal â†’ âˆž
   - Information-theoretic justification

4. **Maintains exact coverage guarantees**
   - Unlike other decomposition methods
   - Distribution-free, finite-sample valid

5. **Single forward pass at test time**
   - No ensembles, no dropout, no TTA needed
   - Computational efficiency for real-time video

### Advantages Over Existing Methods

| Method | Needs Ensemble | Needs Retraining | Has Guarantees | Decomposes | Single Pass |
|--------|---------------|------------------|----------------|------------|-------------|
| Standard CP | âŒ | âŒ | âœ… | âŒ | âœ… |
| EPICSCORE | âœ… | âŒ | âœ… | âœ… | âŒ |
| Deep Ensembles | âœ… | âœ… | âŒ | âœ… | âŒ |
| MC Dropout | âŒ | âŒ | âŒ | âœ… | âŒ |
| **CACD (Ours)** | âŒ | âŒ | âœ… | âœ… | âœ… |

---

## Part VII: Implementation Roadmap

### Phase 1: Core Framework (Week 1-2)
- [ ] Implement HeteroscedasticMLP
- [ ] Implement InfluenceComputer
- [ ] Create base CACD class
- [ ] Unit tests for each component

### Phase 2: Toy Problems (Week 3)
- [ ] Generate 2D heteroscedastic data
- [ ] Train and evaluate CACD
- [ ] Create visualizations
- [ ] Compare with baselines

### Phase 3: MOT17 Integration (Week 4-5)
- [ ] Extract features from YOLO
- [ ] Apply CACD to Track 25
- [ ] Temporal consistency loss
- [ ] Full pipeline evaluation

### Phase 4: Paper Writing (Week 6-8)
- [ ] Introduction and related work
- [ ] Method section with proofs
- [ ] Experimental results
- [ ] Figures and tables

---

## Part VIII: Critical Questions & Clarifications Needed

### Technical Clarifications Needed

1. **Calibration Set Construction**
   - Should we use random split or strategic sampling?
   - How to ensure coverage of feature space?
   - Optimal size: 10% of data? 20%?

2. **Feature Extraction**
   - Which layer of pre-trained model?
   - Raw features or processed (PCA, etc.)?
   - Normalization strategy?

3. **Hyperparameter Selection**
   - Bandwidth h: Cross-validation or analytical?
   - Network architecture: Depth vs width?
   - Training phases: Optimal epochs for each?

4. **Influence Approximation**
   - Exact leave-one-out or gradient-based?
   - Computational budget constraints?
   - Caching strategy for efficiency?

5. **Temporal Modeling**
   - How much history to consider?
   - Exponential decay of past influence?
   - Handling missing observations?

### Experimental Design Questions

1. **Baseline Selection**
   - Which version of EPICSCORE to compare?
   - Include ensemble methods despite cost?
   - How many seeds for stability?

2. **Evaluation Metrics**
   - How to measure "decomposition quality"?
   - Ground truth for epistemic in real data?
   - Statistical tests for significance?

3. **Computational Budget**
   - Target inference time?
   - Memory constraints?
   - GPU requirements?

### Paper Strategy Questions

1. **Venue Selection**
   - CVPR: Emphasize video/vision applications?
   - NeurIPS: Emphasize theoretical contributions?
   - ICML: Balance of both?

2. **Story Framing**
   - Lead with theory or applications?
   - Emphasize efficiency or decomposition quality?
   - How much space for proofs vs experiments?

3. **Figure Allocation (8 pages)**
   - Fig 1: Architecture diagram
   - Fig 2: Toy problem results
   - Fig 3: MOT17 tracking results
   - Fig 4: Ablation study
   - Fig 5: Comparison table/plot
   - What else is crucial?

### Implementation Uncertainties

1. **Numerical Stability**
   - Log variance can explode/vanish - clipping strategy?
   - Gradient flow in heteroscedastic loss?
   - Influence computation condition number?

2. **Edge Cases**
   - Empty calibration regions?
   - All calibration points equidistant?
   - Constant predictions (ÏƒÂ² = 0)?

3. **Scaling Issues**
   - Large n_cal (>10000)?
   - High-dimensional features (d > 1000)?
   - Video with 1000+ frames?

### Validation Requirements

1. **Theoretical Validation**
   - Finite-sample coverage: How to prove maintained?
   - Orthogonality: Empirical verification sufficient?
   - Convergence rates: Match theory?

2. **Empirical Validation**
   - How many random seeds?
   - Cross-dataset generalization?
   - Failure mode analysis?

### Future Extensions (Post-Paper)

1. **Active Learning Integration**
   - Select calibration points optimally?
   - Online calibration updates?
   - Adaptive Î± based on epistemic?

2. **Multi-Task/Multi-Modal**
   - Shared calibration across tasks?
   - Cross-modal uncertainty transfer?
   - Hierarchical decomposition?

3. **Theoretical Extensions**
   - PAC-Bayes bounds?
   - Rademacher complexity?
   - VC dimension analysis?

---

## Part IX: Key Implementation Notes

### Critical Implementation Details

1. **Variance Head Initialization**
   ```python
   # Start with low variance to ensure stable training
   bias_init = -2.0  # log(0.135) - conservative start
   weight_init = Normal(0, 0.01)  # Small weights
   ```

2. **Gradient Clipping**
   ```python
   clip_grad_norm_(mu_params, max_norm=1.0)
   clip_grad_norm_(sigma_params, max_norm=0.5)  # More conservative for variance
   ```

3. **Numerical Stability**
   ```python
   # Avoid log(0) and division by zero
   sigma_squared = torch.clamp(sigma_squared, min=1e-6, max=10.0)
   normalized_score = score / (sigma + 1e-8)
   ```

4. **Efficient Influence Computation**
   ```python
   # Cache kernel matrix
   K = compute_kernel_matrix(cal_features)  # O(nÂ²) once
   influences = K @ (scores - quantile)  # O(n) per test point
   ```

5. **Temporal Smoothing**
   ```python
   # Exponential moving average for video
   smoothed_epis[t] = Î± * raw_epis[t] + (1-Î±) * smoothed_epis[t-1]
   # But NOT for aleatoric (can jump at occlusions)
   ```

---

## Conclusion

### What We're Actually Building: CACD

To be absolutely clear for future reference:
- **We are implementing the CACD framework described in Part II**
- **Part I ideas were just initial brainstorming** - included for historical context
- **CACD synthesizes the best elements**: heteroscedastic meta-predictor (from Idea 5) + influence functions (from Idea 3) + temporal consistency (from Idea 2)
- **Our novel contribution**: Using calibration data ACTIVELY through a learned heteroscedastic neural network

This CACD framework represents a significant advance in uncertainty quantification by:
1. Making calibration data ACTIVE rather than passive
2. Providing interpretable uncertainty decomposition
3. Maintaining theoretical guarantees
4. Being computationally efficient
5. Working with ANY pre-trained model

The key insight is that the calibration set contains rich information about uncertainty structure that current methods completely ignore. By learning from this structure, we can decompose uncertainty without ensembles or retraining.

**Next Steps**:
1. Implement toy problem to validate core CACD concept
2. Test on MOT17 Track 25 for video validation
3. Write paper emphasizing novel theoretical contributions of CACD
4. Create compelling visualizations showing CACD's decomposition quality

---

*This document captures the complete CACD framework. Part I shows the evolution of thinking, Part II contains what we're actually implementing. All mathematical details, implementation strategies, and experimental designs are included. Points needing clarification are explicitly marked in Part VIII for resolution during toy problem implementation.*